# app/services/log_parser.py (DEFINITIVE, WITH SELF-FILTERING)
import logging
import json
import time
import os
import asyncio
import psutil  # Used to get the server's own IP addresses
import socket  # Used for the address family constant

from datetime import datetime
from sqlalchemy.orm import Session
from app.database import SessionLocal
from app import models
from app.routers.connection_manager import manager

logger = logging.getLogger(__name__)
SURICATA_LOG_FILE = "/var/log/suricata/eve.json"


def get_server_ips():
    """
    Retrieves all non-loopback IPv4 addresses of the server to filter out
    self-generated alerts (e.g., from Nmap scans).
    """
    server_ips = set()
    try:
        # Add the loopback address by default, as we never want alerts from it
        server_ips.add("127.0.0.1")
        
        # psutil returns a dictionary of network interfaces and their addresses
        for interface, addrs in psutil.net_if_addrs().items():
            for addr in addrs:
                # We are only interested in IPv4 addresses
                if addr.family == socket.AF_INET:
                    server_ips.add(addr.address)
        return server_ips
    except Exception as e:
        logger.error(f"Could not retrieve server IP addresses: {e}")
        # Fallback to just the loopback address
        return {"127.0.0.1"}

# --- NEW: Get the server's IPs on startup and store them in a global variable ---
SERVER_IPS = get_server_ips()
# --------------------------------------------------------------------------------


def process_log_entry(line: str):
    """Parse a single JSON log line and save it to the DB, ignoring self-generated alerts."""
    try:
        log = json.loads(line)

        # We are only interested in 'alert' events.
        if log.get('event_type') != 'alert':
            return

        # --- NEW: FILTERING LOGIC ---
        # Get the source IP from the alert
        source_ip = log.get('src_ip')

        # If the alert was generated by one of the server's own IPs, ignore it.
        #if source_ip in SERVER_IPS:
            # We use a DEBUG log level here so we can see what's being ignored if we need to,
            # without cluttering the main log.
            #logger.debug(f"Ignoring self-generated alert from {source_ip}: {log.get('alert', {}).get('signature')}")
            #return # Exit the function immediately
        # --- END OF FILTERING LOGIC ---


        alert_data = log.get('alert', {})
        timestamp_obj = datetime.fromisoformat(log.get('timestamp').replace("Z", "+00:00"))

        new_alert = models.SecurityAlert(
            timestamp=timestamp_obj,
            source_ip=source_ip, # We already have it from above
            source_port=log.get('src_port'),
            destination_ip=log.get('dest_ip'),
            destination_port=log.get('dest_port'),
            protocol=log.get('proto'),
            severity=alert_data.get('severity', 3),
            signature=alert_data.get('signature'),
            event_type=log.get('event_type'),
            raw_log=line
        )

        with SessionLocal() as db:
            db.add(new_alert)
            db.commit()
            logger.info(f"âœ… Real-Time Alert: '{new_alert.signature}' saved to database.")

            alert_payload = {
                "type": "new_alert",
                "data": {
                    "timestamp": new_alert.timestamp.isoformat(),
                    "signature": new_alert.signature,
                    "severity": new_alert.severity,
                    "source_ip": new_alert.source_ip,
                    "destination_ip": new_alert.destination_ip,
                    "destination_port": new_alert.destination_port
                }
            }
            # This broadcast now only happens for external alerts
            asyncio.run(manager.broadcast(json.dumps(alert_payload)))

    except Exception as e:
        logger.error(f"Failed to process/save alert: '{line[:100]}...'. Error: {e}", exc_info=True)
        if 'db' in locals() and db.in_transaction():
            db.rollback()


def start_log_monitoring():
    """Main entry point for the log parsing background thread."""
    logger.info("Log monitoring service starting (Real-Time Dynamic Mode).")
    # Log the IPs that will be ignored, so you can confirm it's working as expected.
    logger.info(f"Self-filtering is active. Alerts originating from the following server IPs will be ignored: {list(SERVER_IPS)}")
    
    time.sleep(5) 
    
    try:
        # Jump to the end of the file so we only process new alerts.
        with open(SURICATA_LOG_FILE, 'r') as f:
            f.seek(0, os.SEEK_END)
            last_pos = f.tell()
    except FileNotFoundError:
        last_pos = 0
        logger.warning(f"Log file not found at startup: {SURICATA_LOG_FILE}. Will keep trying.")

    while True:
        try:
            with open(SURICATA_LOG_FILE, 'r') as f:
                current_size = os.fstat(f.fileno()).st_size
                if current_size < last_pos:
                    last_pos = 0 # Log file was rotated or truncated
                
                f.seek(last_pos)
                for line in f:
                    if line.strip():
                        process_log_entry(line.strip())
                last_pos = f.tell()
        except FileNotFoundError:
            last_pos = 0
            time.sleep(2)
            continue
        except Exception as e:
            logger.error(f"Error in main log monitoring loop: {e}. Retrying...", exc_info=True)

        time.sleep(1)
